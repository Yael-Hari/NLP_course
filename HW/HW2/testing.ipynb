{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from preprocessing import SentencesEmbeddingDataset\n",
    "from model2_nn import train_and_plot\n",
    "from torch.nn.utils import rnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam\n",
    "from train_loop_model3 import train_and_plot_LSTM\n",
    "from utils import remove_padding\n",
    "from model3_comp import LSTM_NER_NN\n",
    "from utils import print_epoch_details, remove_padding\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding...\n"
     ]
    }
   ],
   "source": [
    "ds = SentencesEmbeddingDataset(vec_dim=500, list_embedding_paths=['glove-twitter-200', 'word2vec-google-news-300'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader, dev_loader = ds.get_data_loaders(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim=32 | activation=Tanh() | num_layers=1\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "num_epochs = 30\n",
    "hidden_dim = 32\n",
    "embedding_dim = ds.vec_dim\n",
    "lr = 0.001\n",
    "# activation = nn.ReLU()\n",
    "# activation = nn.Sigmoid()\n",
    "activation = nn.Tanh()\n",
    "num_layers = 1\n",
    "model_save_path = (\n",
    "    f\"LSTM_model_stateDict_batchSize_{batch_size}_hidden_{hidden_dim}_lr_{lr}.pt\"\n",
    ")\n",
    "\n",
    "LSTM_model = LSTM_NER_NN(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dim=hidden_dim,\n",
    "    model_save_path=model_save_path,\n",
    "    activation=activation,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "optimizer = Adam(params=LSTM_model.parameters(), lr=lr)\n",
    "\n",
    "weight_0 = (ds.train_num_label_1 / ds.train_num_label_0)\n",
    "weight_1 = 1 - weight_0\n",
    "labels_weights = [weight_0, weight_1]\n",
    "labels_weights = [0.1, 0.9]\n",
    "loss_func = nn.CrossEntropyLoss(weight=torch.tensor(labels_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n",
      "Epoch: 1/30 | Train Avg Loss: 0.076 | Train Accuracy: 0.940 | Train F1: 0.370\n",
      "[[57827.  1743.]\n",
      " [ 2048.  1112.]]\n",
      "Epoch: 2/30 | Train Avg Loss: 0.062 | Train Accuracy: 0.927 | Train F1: 0.511\n",
      "[[55808.  3762.]\n",
      " [  786.  2374.]]\n",
      "Epoch: 3/30 | Train Avg Loss: 0.058 | Train Accuracy: 0.936 | Train F1: 0.560\n",
      "[[56179.  3391.]\n",
      " [  615.  2545.]]\n",
      "Epoch: 4/30 | Train Avg Loss: 0.057 | Train Accuracy: 0.942 | Train F1: 0.594\n",
      "[[56459.  3111.]\n",
      " [  509.  2651.]]\n",
      "Epoch: 5/30 | Train Avg Loss: 0.055 | Train Accuracy: 0.948 | Train F1: 0.626\n",
      "[[56756.  2814.]\n",
      " [  436.  2724.]]\n",
      "Epoch: 6/30 | Train Avg Loss: 0.054 | Train Accuracy: 0.951 | Train F1: 0.646\n",
      "[[56897.  2673.]\n",
      " [  379.  2781.]]\n",
      "Epoch: 7/30 | Train Avg Loss: 0.053 | Train Accuracy: 0.957 | Train F1: 0.678\n",
      "[[57207.  2363.]\n",
      " [  327.  2833.]]\n",
      "Epoch: 8/30 | Train Avg Loss: 0.053 | Train Accuracy: 0.961 | Train F1: 0.701\n",
      "[[57401.  2169.]\n",
      " [  287.  2873.]]\n",
      "Epoch: 9/30 | Train Avg Loss: 0.052 | Train Accuracy: 0.964 | Train F1: 0.718\n",
      "[[57573.  1997.]\n",
      " [  269.  2891.]]\n",
      "Epoch: 10/30 | Train Avg Loss: 0.051 | Train Accuracy: 0.968 | Train F1: 0.744\n",
      "[[57797.  1773.]\n",
      " [  238.  2922.]]\n",
      "Epoch: 11/30 | Train Avg Loss: 0.050 | Train Accuracy: 0.971 | Train F1: 0.764\n",
      "[[57966.  1604.]\n",
      " [  213.  2947.]]\n",
      "Epoch: 12/30 | Train Avg Loss: 0.050 | Train Accuracy: 0.974 | Train F1: 0.782\n",
      "[[58111.  1459.]\n",
      " [  193.  2967.]]\n",
      "Epoch: 13/30 | Train Avg Loss: 0.050 | Train Accuracy: 0.975 | Train F1: 0.792\n",
      "[[58187.  1383.]\n",
      " [  184.  2976.]]\n",
      "Epoch: 14/30 | Train Avg Loss: 0.049 | Train Accuracy: 0.977 | Train F1: 0.802\n",
      "[[58283.  1287.]\n",
      " [  180.  2980.]]\n",
      "Epoch: 15/30 | Train Avg Loss: 0.049 | Train Accuracy: 0.979 | Train F1: 0.822\n",
      "[[58440.  1130.]\n",
      " [  164.  2996.]]\n",
      "Epoch: 16/30 | Train Avg Loss: 0.048 | Train Accuracy: 0.982 | Train F1: 0.839\n",
      "[[58572.   998.]\n",
      " [  158.  3002.]]\n",
      "Epoch: 17/30 | Train Avg Loss: 0.048 | Train Accuracy: 0.982 | Train F1: 0.844\n",
      "[[58609.   961.]\n",
      " [  154.  3006.]]\n",
      "Epoch: 18/30 | Train Avg Loss: 0.048 | Train Accuracy: 0.982 | Train F1: 0.844\n",
      "[[58605.   965.]\n",
      " [  148.  3012.]]\n",
      "Epoch: 19/30 | Train Avg Loss: 0.048 | Train Accuracy: 0.984 | Train F1: 0.854\n",
      "[[58684.   886.]\n",
      " [  144.  3016.]]\n",
      "Epoch: 20/30 | Train Avg Loss: 0.048 | Train Accuracy: 0.985 | Train F1: 0.864\n",
      "[[58757.   813.]\n",
      " [  141.  3019.]]\n",
      "Epoch: 21/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.986 | Train F1: 0.872\n",
      "[[58816.   754.]\n",
      " [  135.  3025.]]\n",
      "Epoch: 22/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.987 | Train F1: 0.881\n",
      "[[58879.   691.]\n",
      " [  130.  3030.]]\n",
      "Epoch: 23/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.987 | Train F1: 0.881\n",
      "[[58886.   684.]\n",
      " [  134.  3026.]]\n",
      "Epoch: 24/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.988 | Train F1: 0.888\n",
      "[[58935.   635.]\n",
      " [  128.  3032.]]\n",
      "Epoch: 25/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.989 | Train F1: 0.897\n",
      "[[58995.   575.]\n",
      " [  122.  3038.]]\n",
      "Epoch: 26/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.989 | Train F1: 0.901\n",
      "[[59024.   546.]\n",
      " [  119.  3041.]]\n",
      "Epoch: 27/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.990 | Train F1: 0.906\n",
      "[[59060.   510.]\n",
      " [  118.  3042.]]\n",
      "Epoch: 28/30 | Train Avg Loss: 0.047 | Train Accuracy: 0.990 | Train F1: 0.910\n",
      "[[59088.   482.]\n",
      " [  118.  3042.]]\n",
      "Epoch: 29/30 | Train Avg Loss: 0.046 | Train Accuracy: 0.991 | Train F1: 0.915\n",
      "[[59120.   450.]\n",
      " [  118.  3042.]]\n",
      "Epoch: 30/30 | Train Avg Loss: 0.046 | Train Accuracy: 0.991 | Train F1: 0.918\n",
      "[[59146.   424.]\n",
      " [  118.  3042.]]\n"
     ]
    }
   ],
   "source": [
    "val_loader=None\n",
    "# -------\n",
    "# GPU\n",
    "# -------\n",
    "# First checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Training on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, training on CPU.\")\n",
    "LSTM_model.to(device)\n",
    "\n",
    "# ----------------------------------\n",
    "# Epoch Loop\n",
    "# ----------------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    data_loaders = {\"train\": train_loader}\n",
    "    if val_loader:\n",
    "        data_loaders[\"validate\"] = val_loader\n",
    "\n",
    "    # prepare for evaluate\n",
    "    num_classes = LSTM_model.num_classes\n",
    "    train_confusion_matrix = np.zeros([num_classes, num_classes])\n",
    "    val_confusion_matrix = None\n",
    "    if val_loader:\n",
    "        val_confusion_matrix = np.zeros([num_classes, num_classes])\n",
    "    train_loss_batches_list = []\n",
    "\n",
    "    for loader_type, data_loader in data_loaders.items():\n",
    "        # num_of_batches = len(data_loader)\n",
    "\n",
    "        for batch_num, (sentences, labels, sen_lengths) in enumerate(data_loader):\n",
    "            # if training on gpu\n",
    "            sentences, labels, sen_lengths = (\n",
    "                sentences.to(device),\n",
    "                labels.to(device),\n",
    "                sen_lengths.to(device),\n",
    "            )\n",
    "\n",
    "            # forward\n",
    "            outputs = LSTM_model(sentences, sen_lengths)\n",
    "\n",
    "            # labels\n",
    "            packed_labels = pack_padded_sequence(\n",
    "                labels, sen_lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            unpacked_labels, labels_lengths = pad_packed_sequence(\n",
    "                packed_labels, batch_first=True\n",
    "            )\n",
    "            unpadded_labels = remove_padding(unpacked_labels, labels_lengths).long()\n",
    "            labels_one_hot = one_hot(unpadded_labels, num_classes=num_classes)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_func(outputs, labels_one_hot.float())\n",
    "\n",
    "            if loader_type == \"train\":\n",
    "                train_loss_batches_list.append(loss.detach().cpu())\n",
    "                # backprop\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # predictions\n",
    "            preds = outputs.argmax(dim=1).clone().detach().cpu()\n",
    "            y_true = np.array(unpadded_labels.cpu().view(-1).int())\n",
    "            y_pred = np.array(preds.view(-1))\n",
    "            n_preds = len(y_pred)\n",
    "            for i in range(n_preds):\n",
    "                if loader_type == \"train\":\n",
    "                    train_confusion_matrix[y_true[i]][y_pred[i]] += 1\n",
    "                if loader_type == \"validate\":\n",
    "                    val_confusion_matrix[y_true[i]][y_pred[i]] += 1\n",
    "\n",
    "        print_epoch_details(\n",
    "            num_epochs,\n",
    "            epoch,\n",
    "            train_confusion_matrix,\n",
    "            train_loss_batches_list,\n",
    "            val_confusion_matrix,\n",
    "            loader_type,\n",
    "        )\n",
    "        if loader_type == \"train\":\n",
    "            print(train_confusion_matrix)\n",
    "        if loader_type == \"validate\":\n",
    "            print(val_confusion_matrix)\n",
    "\n",
    "torch.save(LSTM_model.state_dict(), LSTM_model.model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0530), tensor(0.9470)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
